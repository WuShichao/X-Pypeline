#!/usr/bin/env python

from xpipeline.core.xtimeseries import XTimeSeries
from xpipeline.core.xdetector import Detector
from xpipeline.core.xdetector import compute_antenna_patterns
from xpipeline.likelihood.xlikelihood import XLikelihoodMap
from xpipeline.core.xtimefrequencymap import XTimeFrequencyMapDict
from xpipeline.core.xtimefrequencymap import csc_XSparseTimeFrequencyMap
from xpipeline.setuputils import log
from gwpy.table import EventTable

import pandas
import numpy
import argparse

def parse_commandline():
    parser = argparse.ArgumentParser(description="This executable processes "
                                                 "injection, off source "
                                                 "and on source trials "
                                                 "X-Pipeline triggered search jobs.")
    # Define groups of flags to make the help output ore useful
    requiredNamed = parser.add_argument_group('required named arguments')
    requiredNamed.add_argument("-j", "--job-type", help=
                               """
                               Are you analyzing an injection
                               or a background trial
                               """,
                               required=True)
    requiredNamed.add_argument("-p", "--parameter-file", help=
                               """
                               This file is usually generated by
                               xpipeline-workflow and is in the input
                               directory. Example:
                               input/parameters_off_source_0.txt
                               """,
                               required=True)
    requiredNamed.add_argument("-t", "--event-number", help=
                               """
                               This indicates what background timeslide
                               and timestamp to use
                               The file that the event-number picks from
                               is usually listed in the params file and
                               in a two detector analysis has the form
                               1126259462.0 0 0
                               (timestamp, seconds to slide detector 1,
                               slide detector 2
                               """,
                               required=True, type=int)


    args = parser.parse_args()

    return args

args = parse_commandline()

logger = log.Logger('XPIPELINE: Analysis {0}'.format(args.job_type))

# Read parameters
parameters = dict(line.rstrip('\n').split(':', 1) for line in open(args.parameter_file))


block_time = int(parameters['blocktime'])

minimum_frequency = int(parameters['minimumfrequency'])
maximum_frequency = int(parameters['maximumfrequency'])

frame_file = parameters['frameCacheFile']
whitening_length = int(parameters['whiteningtime'])
transient_time = 4.0

circ_time_slides = int(parameters['circtimeslidestep'])
event_file = parameters['eventFileName']

on_source_end_offset = int(parameters['onsourceendoffset'])
on_source_begin_offset = int(parameters['onsourcebeginoffset'])

sky_positions = parameters['skyPositionList']
sky_positions = EventTable.read(sky_positions, format='ascii',
                                names=['theta', 'phi', 'probability', 'area'])

seed = int(parameters['seed'])
fft_lengths = float(parameters['analysistimes'])
likelihoods = parameters['likelihoodtype'].split(',')

channel_names = parameters['channelFileName']
channel_names = [chan.rstrip('\n').split(' ')[0]
                 for chan in open(channel_names)]

sample_frequency = float(parameters['samplefrequency'])

numpy.random.seed(seed)


# presets
events = EventTable.read(event_file, format='ascii')
event = events[args.event_number]
event_time = event[0]

logger.info("""
You are processing event {0} which corresponds to gps time
{1} and slides of {2}
""".format(args.event_number, event_time, event[1:]))

import pdb
pdb.set_trace()

# likelihoods to analze
try:
    data = XTimeSeries.retrieve_data(event_time, block_time,
                                     channel_names, sample_frequency)
except:
    data = XTimeSeries.read('examples/GW150914.gwf',
                            channels=channels)

# seconds to slide tf-maps (so called internal time slide
slides = {}
if circ_time_slides is not None:
    nchannels = len(channel_names)
    if nchannels == 1:
        slides[channel_names[0]] = numpy.zeros(1)
    elif nchannels == 2:
        internal_slides = numpy.arange(0, (block_time - 2*transient_time -
                                           circ_time_slides),
                                       circ_time_slides).astype(int)
        slides[channel_names[0]] = numpy.zeros(internal_slides.size)
        slides[channel_names[1]] = internal_slides
    elif nchannels == 3:
        internal_slides = numpy.arange(0, (block_time / 2 - transient_time -
                                           circ_time_slides),
                                       circ_time_slides).astype(int)
        slides[channel_names[0]] = numpy.zeros(internal_slides.size)
        slides[channel_names[1]] = internal_slides
        slides[channel_names[2]] = -internal_slides
    else:
        raise ValueError("The circular time slides are not "
                         "implemented for the requested "
                         "number of detectors: {0}".format(nchannels))
else:
    slides[channel_names[0]] = numpy.zeros(1)

injection=False
if injection:
    dosomething=0

asds = data.asd(whitening_length)
whitened_timeseries = data.whiten(asds)

hanford = Detector('H1')
livingston = Detector('L1')

all_clusters = []

for fft_length in fft_lengths:
    for sky_postion in sky_postions:
        phi = sky_postion[0]; theta = sky_postion[1]

        time_shift = (hanford.time_delay_from_earth_center_phi_theta([phi], [theta]) -
                      livingston.time_delay_from_earth_center_phi_theta([phi], [theta])
                     )

        # shift data appropriately
        whitened_timeseries['L1:GDS-CALIB_STRAIN'].shift(time_shift[0])

        # NOTE we calculate everything from here to the loop over
        # internal time slides for the non-slide maps
        # this way when we find pizels for the slide maps
        # if we undo the slide to the non-slide maps values
        # then we do not need to recalculate say likelihood maps again
        # and again

        ####################################################
        # Create TF maps and Likelihood Maps               #
        ####################################################

        # Make a spectrogram that contains phase information
        fft_grams = whitened_timeseries.fftgram(fft_length)

        time_shift = (hanford.time_delay_from_earth_center_phi_theta([phi], [theta]) -
                      livingston.time_delay_from_earth_center_phi_theta([phi], [theta])
                     )

        ####################################################
        # Find loud pixels and down select maps            #
        ####################################################
        # Create spectrogram that contains info of the energy in each pixel
        energy_maps = fft_grams.abs()
        # for reference we will create a coherent map out of all pixels before
        # only making maps with the reduced set of pixels
        full_coh_energy_map = energy_maps.to_coherent()

        # Turn off the bottom 99 percent of pixels (i.e. set to 0)
        energy_maps_zeroed = energy_maps.blackout_pixels(99)

        # Add maps together and then back extract the t-f indices
        # of the pixels, these will represent all t-f pixels used
        # in our analysis and new sparse tf maps will need
        # to be made for the individual ifos
        coh_energy_maps_zeroed = energy_maps_zeroed.to_coherent()

        # find nonzero pixels indicies
        tf_indices = coh_energy_maps_zeroed.nonzero()

        # If we are doing internal time slides we should register the time bins
        # as these are the only things that will be shifted for any
        # detectors
        tindex = {k : tf_indices[0] for k in energy_maps}
        findex = {k : tf_indices[1] for k in energy_maps}

        # Obtain individual ifo energies
        no_slide_energies = {k : v.value[tf_indices]
                             for k, v in energy_maps.items()}

        # Now that we have the pixels that are in each map
        # let's reconstruc tthe sparse tf map
        # Now with pixels from both detectors
        energy_maps_zeroed = energy_maps.to_sparse(tindex, findex)

        # Recreate individual sparse projected tfmaps
        #sparse_projected_fftgrams = {k : v.to_sparse(tindex, findex)
        #                             for k,v in projected_fftmaps.items()}

        # make a note of the final time bin
        final_time_bin_idx = energy_maps['H1:GDS-CALIB_STRAIN'].shape[0]

        coord_dim_array = energy_maps['H1:GDS-CALIB_STRAIN'].shape
        # Alright Now that we have all this information about all the pixels
        # we must group them together in nearby "clusters" of pixels
        # these clusters will represent our possible gravitational
        # wave canididates
        for slide in slides.values():

            # slide time indices
            for detector, second in slide.items():
                # check to make sure this tfmap is even being slide by any seconds
                if second:
                    number_of_time_bins_shifted = second *2048

                    # take the time indices and shift them
                    time_idx_shifted = numpy.mod(tindex[detector] +
                                                 number_of_time_bins_shifted,
                                                 final_time_bin_idx)

                    # Reconstruct sparse matrix
                    energy_maps_zeroed[detector] = csc_XSparseTimeFrequencyMap(
                                                       (no_slide_energies[detector],
                                                        (time_idx_shifted, findex[detector])
                                                       ),
                                                       shape=coord_dim_array,
                                                       tindex=time_idx_shifted,
                                                       findex=findex[detector])

            # Add new maps together
            coh_energy_maps_zeroed = energy_maps_zeroed.to_coherent()

            # find nonzero pixels indicies
            tf_indices = coh_energy_maps_zeroed.nonzero()

            tindex_slide = {k : tf_indices[0] for k in energy_maps}
            findex_slide = {k : tf_indices[1] for k in energy_maps}
            # Perform the time slide again to joint pixels
            for detector, second in slide.items():
                # check to make sure this tfmap is even being slide by any seconds
                if second:
                    tindex_slide[detector] = numpy.mod(tindex_slide[detector] +
                                                       number_of_time_bins_shifted,
                                                       final_time_bin_idx)

            # Obtain individual ifo energies
            maps_to_cluster = energy_maps.to_sparse(tindex, findex)
            cluster = maps_to_cluster.cluster()
            all_clusters.append(cluster)
            #final_clusters = clusters.nlargest(248, 'energy_of_cluster')
            #final_clusters['fft_length'] = fft_length
            #final_clusters['phi'] = phi
            #final_clusters['theta'] = theta
            #slide_string = '~'.join([str(islide) for islide in slide.values()])
            #final_clusters['slide'] = slide_string
            #cluster_this_fftlength = cluster_this_fftlength.append(final_clusters)

    #all_clusters = all_clusters.append(
    #                    cluster_this_fftlength.groupby('slide').apply(lambda grp: grp.nlargest(248, 'energy_of_cluster')).reset_index(drop=True)
    #                   )

#all_clusters = EventTable.from_pandas(all_clusters)
#all_clusters.convert_unicode_to_bytestring()
#all_clusters.write('triggers.hdf5', format='hdf5', path='offsource', append=True)
